


pip install pandas numpy scikit-learn matplotlib seaborn



import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


location1 = pd.read_csv('Location1.csv')
location2 = pd.read_csv('Location2.csv')
location3 = pd.read_csv('Location3.csv')
location4 = pd.read_csv('Location4.csv')


location1.tail()


location1.head()


location1.head(10)


location2.tail(10)


location3.head(10
        )


location4.tail(10)


location4.head(10)


location1['Location']='Location1'
location2['Location']='Location2'

location3['Location']='Location3'
location4['Location']='Location4'

merged_data = pd.concat([location1, location2, location3, location4], ignore_index=True)

merged_data.tail()


merged_data.to_csv('merged_locations.csv', index=False)


merged_data.info()


merged_data.info()


merged_data.info()


merged_data.to_csv('merged_locations.csv', index=False)


merged_data.info()


merged_data.head()


merged_data.tail()


merged_data.describe().T


merged_data.isnull().sum()


merged_data.duplicated().sum()


# Encode the categorical variables
merged_data = pd.get_dummies(merged_data, columns=['Location'], drop_first=True)

merged_data.head()


dummy_cols = [col for col in merged_data.columns if col.startswith('Location_')]
merged_data[dummy_cols] = merged_data[dummy_cols].astype(int)



merged_data.info()


merged_data.info()


merged_data.columns


merged_data.drop('Time', axis=1, inplace=True)


merged_data.head()


merged_data.head()


merged_data.tail()





# lets get only numerical colums 
numerical_columns=merged_data.select_dtypes(include=['number']).columns
numerical_columns


merged_data.shape


# lets dive into EDA by plotting graphs
#lets plot using subplots for distruibuton of data of all numerical columns 
fig, axes =plt.subplots(4,3,figsize=(15,15))
axes=axes.flatten()
for i,col in enumerate(numerical_columns):
    sns.histplot(merged_data[col],ax=axes[i])
    axes[i].set_title(col)
plt.tight_layout()
plt.show()


merged_data.info()


#lets create a box plot for the same data 
fig,axes=plt.subplots(4,3,figsize=(15,15))
axes=axes.flatten()
for i,col in enumerate(numerical_columns):
    sns.boxplot(merged_data[col],ax=axes[i])
    axes[i].set_title(col)
plt.tight_layout()
plt.show()

    


merged_data.plot.scatter(x='temperature_2m', y='Power', c='DarkBlue')
plt.xlabel('Temperature')
plt.ylabel('Power')
plt.title('Temperature vs Power')
plt.grid(True)
plt.show()



fig, axes = plt.subplots(3, 4, figsize=(14, 6))
axes = axes.flatten()
for i, col in enumerate(numerical_columns.drop('Power')):
    sns.scatterplot(x=col, y='Power', data=merged_data, ax=axes[i])
    axes[i].set_title(f'Power vs {col}')
    axes[i].set_xlabel(col)
    axes[i].set_ylabel('Power')
plt.tight_layout()
plt.show()



correlation = merged_data[numerical_columns].corr()
plt.figure(figsize=(10, 10))
sns.heatmap(correlation, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()


merged_data.info()


merged_data.head()





X=merged_data.drop('Power',axis=1)
y=merged_data['Power']
# here basically we are seperating out features and target 
# features are independent and power is what we need to predict as y(convention)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Scale the numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


# Train with Linear Regression Model
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# initialize the model
model = LinearRegression()
# Train the model
model.fit(X_train, y_train)


# Make the predictions
y_pred = model.predict(X_test)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Absolute Error: {mae}')
print(f'Mean Squared Error: {mse}')
print(f'R Squared Score: {r2}')


# Train using Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor

# Initialize the model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Train the RF Model
rf_model.fit(X_train, y_train)


# Let's predict
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f'RF MAE: {mae_rf}')
print(f'RF MSE: {mse_rf}')
print(f'RF R2 Score: {r2_rf}')


get_ipython().getoutput("pip install xgboost")


# Train using XGBoost Regressor Model
get_ipython().getoutput("pip install xgboost")



# Train using XGBoost Regressor Model
from xgboost import XGBRegressor
# Initialize the model
xgb_model = XGBRegressor()
# Train the model
xgb_model.fit(X_train, y_train)



# Prediction
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
r2_xgb = r2_score(y_test, y_pred_xgb)

print(f'XGB MAE: {mae_xgb}')
print(f'XGB MSE: {mse_xgb}')
print(f'XGB R2: {r2_xgb}')


# Hyperparameter tuning
from sklearn.model_selection import GridSearchCV
# Define the parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(
    estimator=XGBRegressor(random_state=42),
    param_grid=param_grid,
    scoring='neg_mean_absolute_error',  # Use MAE for evaluation
    cv=3,  # 3-fold cross-validation
    n_jobs=-1  # Use all available CPU cores
)

# Fit the GridSearchCV
grid_search.fit(X_train, y_train)

# Best parameters and best score
print(f'Best Parameters: {grid_search.best_params_}')
print(f'Best MAE: {-grid_search.best_score_}')

# Evaluate the tuned model
best_model = grid_search.best_estimator_
y_pred_tuned = best_model.predict(X_test)

mae_tuned = mean_absolute_error(y_test, y_pred_tuned)
mse_tuned = mean_squared_error(y_test, y_pred_tuned)
r2_tuned = r2_score(y_test, y_pred_tuned)

print(f'Tuned Model Mean Absolute Error (MAE): {mae_tuned}')
print(f'Tuned Model Mean Squared Error (MSE): {mse_tuned}')
print(f'Tuned Model R^2 Score: {r2_tuned}')


get_ipython().getoutput("pip install lightgbm")
    


from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the model
lgb_model = LGBMRegressor(
    num_leaves=31,
    max_depth=-1,  # No limit
    learning_rate=0.1,
    n_estimators=100,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

# Train the model
lgb_model.fit(X_train, y_train)

# Predict
y_pred_lgb = lgb_model.predict(X_test)

# Evaluate
mae_lgb = mean_absolute_error(y_test, y_pred_lgb)
mse_lgb = mean_squared_error(y_test, y_pred_lgb)
r2_lgb = r2_score(y_test, y_pred_lgb)

print(f'LightGBM MAE: {mae_lgb}')
print(f'LightGBM MSE: {mse_lgb}')
print(f'LightGBM R2 Score: {r2_lgb}')



# as we got r2 score of lightgbm is less than xgboost lets try catBoost algo


get_ipython().getoutput("pip install catboost")


from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Initialize the CatBoost model
cat_model = CatBoostRegressor(
    iterations=300,             # like n_estimators
    learning_rate=0.1,
    depth=6,
    loss_function='RMSE',
    verbose=0,                  # set to 100 to see training log every 100 iterations
    random_state=42
)

# Fit the model
cat_model.fit(X_train, y_train)

# Predict
y_pred_cat = cat_model.predict(X_test)

# Evaluate
mae_cat = mean_absolute_error(y_test, y_pred_cat)
mse_cat = mean_squared_error(y_test, y_pred_cat)
r2_cat = r2_score(y_test, y_pred_cat)

print(f'CatBoost MAE: {mae_cat}')
print(f'CatBoost MSE: {mse_cat}')
print(f'CatBoost R2 Score: {r2_cat}')



 
